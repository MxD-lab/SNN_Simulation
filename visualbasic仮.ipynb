{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "visualbasic仮.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOSpkj62wL1IC+DGaJABqc8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MxD-lab/SNN_Simulation/blob/neralnetwork/visualbasic%E4%BB%AE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "146y_HoNTVTG"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import numpy \n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDuBDjy3Tu1O"
      },
      "source": [
        "#シグモイド関数（出力層）\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "#ReLU関数（隠れ層）\n",
        "def relu(x):\n",
        "  return np.maximum(0,x)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwSt8envTxoP"
      },
      "source": [
        "#交差エントロピー誤差\n",
        "\n",
        "def cross_entropy_error(y,t):\n",
        "  if y.ndim == 1:\n",
        "    t = t.reshape(1,t.size)\n",
        "    y = y.reshape(1,y.size)\n",
        "  \n",
        "  batch_size = y.shape[0]#yの行数\n",
        "  return np.sum(-t*np.log(y)-(1-t)*np.log(1-y))/batch_size\n",
        "\n",
        "\n",
        "  \n",
        "  "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17rruOfDT0on"
      },
      "source": [
        "\"\"\"\n",
        "重み、バイアス、初期設定\n",
        "W1：隠れ層の重み\n",
        "B1:隠れ層のバイアス\n",
        "W2:出力層の重み\n",
        "B2:隠れ層のバイアス\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "W1 = np.array([[0.707760268819045,-0.83735145624508],[-1.21179516971129,-0.426362970984348]])\n",
        "B1 = np.array([-0.311592487617895,0.46169885478039])\n",
        "W2 = np.array([[0.539158501953791],[-0.41755354864446]])\n",
        "B2 = np.array([0.141883319414468])\n",
        "#print(W1)\n",
        "\"\"\"\n",
        "\n",
        "W1 = np.random.rand(2,2)\n",
        "B1 = np.random.rand(2)\n",
        "W2 = np.random.rand(2,1)\n",
        "B2 = np.random.rand(1)\n",
        "#print(W1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ziREoduLT65W",
        "outputId": "bf789810-6755-48e4-9938-76f5b83ccbe8"
      },
      "source": [
        "#活性化関数の微分(relu)u:隠れ層の活性\n",
        "\n",
        "def relu_diff(u):\n",
        "  d = np.zeros_like(u) #xと同じ形状の配列を作成\n",
        "  for i in range(len(u)):\n",
        "    for j in range(len(u[0])):\n",
        "      if u[i][j]>0:\n",
        "        d[i][j] = 1\n",
        "      else: \n",
        "        d[i][j] = 0\n",
        "\n",
        "  return d\n",
        "\n",
        "\"\"\"\n",
        "def relu_diff(u):\n",
        "  d = np.where( x > 0, 1, 0)\n",
        "  return y\n",
        "\"\"\""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef relu_diff(u):\\n  d = np.where( x > 0, 1, 0)\\n  return y\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRoHI9cUT7em"
      },
      "source": [
        "#隠れ層の誤差\n",
        "\"\"\"\n",
        "delta:一個前の誤差（出力層の誤差）\n",
        "w:一個前の重み（出力層の重み）\n",
        "activ:活性化関数の微分\n",
        "\"\"\"\n",
        "def hidden_delta(delta,w,activ):\n",
        "  d = np.zeros((4,2)) #データ数×隠れ層のニューロン数の配列を作成\n",
        "  #d = np.zeros((int(delta),int(w[0])))\n",
        "  for i in range(len(delta)):\n",
        "    for j in range(len(w[0])):\n",
        "      d[i][j] = delta[i]*w[j]*activ[i][j]\n",
        "  return np.array(d)\n",
        "  "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "Wxb_N44Fdd-i",
        "outputId": "60536de3-2763-4ba2-b962-455302040e36"
      },
      "source": [
        "\"\"\"\n",
        "for i in range(100):#Nout\n",
        "  for j in range(100):#Nin\n",
        "    tmp = alpha*W2*N\n",
        "    m_deltaX_accum(i,j) = beta1*m_deltaX_accum(i,j) + beta1coef * (deltaX_accum(i,j) + tmp)\n",
        "    v_deltaX_accum(i, j) = beta2 * v_deltaX_accum(i, j) + beta2coef * (deltaX_accum(i, j) + tmp) * (deltaX_accum(i, j) + tmp)\n",
        "  \n",
        "  m_delta_accum(i) = beta1 * m_delta_accum(i) + beta1coef * delta_accum(i)\n",
        "  v_delta_accum(i) = beta2 * v_delta_accum(i) + beta2coef * delta_accum(i) * delta_accum(i)\n",
        "sqbeta = Sqr((1 - beta2 ^ (epoch + 1)))#Sqrは平方根\n",
        "learning_rate2 = learning_rate / (1 - beta1 ^ (epoch + 1)) * sqbeta\n",
        "epsilon2 = sqbeta * epsilon\n",
        "\n",
        "for i in range(100):\n",
        "  for j in range(100):\n",
        "    w(i, j) = w(i, j) - learning_rate2 * m_deltaX_accum(i, j) / (Sqr(v_deltaX_accum(i, j)) + epsilon2) ' 重みの更新\n",
        "  b(i) = b(i) - learning_rate2 * m_delta_accum(i) / (Sqr(v_delta_accum(i)) + epsilon2)\n",
        "        \n",
        " \"\"\"\n",
        "   "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nfor i in range(100):#Nout\\n  for j in range(100):#Nin\\n    tmp = alpha*W2*N\\n    m_deltaX_accum(i,j) = beta1*m_deltaX_accum(i,j) + beta1coef * (deltaX_accum(i,j) + tmp)\\n    v_deltaX_accum(i, j) = beta2 * v_deltaX_accum(i, j) + beta2coef * (deltaX_accum(i, j) + tmp) * (deltaX_accum(i, j) + tmp)\\n  \\n  m_delta_accum(i) = beta1 * m_delta_accum(i) + beta1coef * delta_accum(i)\\n  v_delta_accum(i) = beta2 * v_delta_accum(i) + beta2coef * delta_accum(i) * delta_accum(i)\\nsqbeta = Sqr((1 - beta2 ^ (epoch + 1)))#Sqrは平方根\\nlearning_rate2 = learning_rate / (1 - beta1 ^ (epoch + 1)) * sqbeta\\nepsilon2 = sqbeta * epsilon\\n\\nfor i in range(100):\\n  for j in range(100):\\n    w(i, j) = w(i, j) - learning_rate2 * m_deltaX_accum(i, j) / (Sqr(v_deltaX_accum(i, j)) + epsilon2) ' 重みの更新\\n  b(i) = b(i) - learning_rate2 * m_delta_accum(i) / (Sqr(v_delta_accum(i)) + epsilon2)\\n        \\n \""
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjCkZSqhT-Sb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "fb0dd74f-7d1d-45f9-cbce-573a6d1ff4bd"
      },
      "source": [
        "\n",
        "i = 0\n",
        "y = np.zeros(100)\n",
        "\n",
        "\n",
        "N = 4#データ数(ミニバッチ)\n",
        "learning_rate = 0.2#学習率\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "epsilon = 0.00000001\n",
        "alpha = 0.0001\n",
        "m = 0\n",
        "v = 0\n",
        "for i in range(100):\n",
        "  \n",
        "  x = np.array([[0,0],[0,1],[1,0],[1,1]])#入力\n",
        "  target =  np.array([[0],[1],[1],[0]]) #教師\n",
        "  \n",
        "\n",
        "  A1 = np.dot(x,W1)+B1\n",
        "  Z1 = relu(A1)#一層目活性化関数かける\n",
        "  #print(\"Z1\")\n",
        "  #print(Z1)\n",
        "  A2 = np.dot(Z1,W2) + B2\n",
        "  Z2 = sigmoid(A2)\n",
        "  #print(\"Z2\")\n",
        "  #print(Z2)\n",
        "  #print(\"target\")\n",
        "  #print(target)\n",
        "  y[i] = cross_entropy_error(Z2,target)#出力層の損失関数\n",
        "  \n",
        "  #print(y)\n",
        "\n",
        "  ###出力層の逆伝搬\n",
        "  ##重み\n",
        "  delta = Z2 - target#誤差\n",
        "  #print(Z2)\n",
        "  #print(\"delta\")\n",
        "  #print(delta)\n",
        "  Z1_t = np.transpose(Z1)\n",
        "  #print(Z1_t)\n",
        "  sum_delta = np.dot(Z1_t,delta)\n",
        "  #print(\"sum_delta\")\n",
        "  #print(sum_delta)\n",
        "  \n",
        "  #print(\"delta_out\")\n",
        "  #delta_out　なんか違う\n",
        "  delta_out =(1/N)*sum_delta\n",
        "  #print(\"delta_out\")\n",
        "  #print(delta_out)\n",
        "  \n",
        "  #print(\"W2\")\n",
        "  #print(W2)\n",
        "  \"\"\"\n",
        "  m = pram_b1*m + (1-pram_b2)*(delta_out + alfa*W2)\n",
        "  v = pram_b2*v + (1-pram_b2)*(delta_out + alfa*W2)**2\n",
        "  m = m/(1-pram_b1**(i+1))\n",
        "  v = v/(1-pram_b2**(i+1))\n",
        "  W2 = W2 - learning_rate*m/(np.sqrt(v)+epsiron)\n",
        "\"\"\"\n",
        "#deltaX_accum(i, j) = deltaX_accum(i, j) + delta(i) * prevLayer.getXInput(selected, j)\n",
        "#getXInput(i,j)　:x(i,j)の値をとってくる  \n",
        "#Nin = prevLayer.getNout；Noutの値をとってくる？\n",
        "#Nsample = prevLayer.getNsample\n",
        "#Nbatch_sample = prevLayer.getNbatchSample     \n",
        "  beta1coef = (1 - beta1) / N\n",
        "  beta2coef = (1 - beta2) / N / N\n",
        "    \n",
        "  for i in 100:#Nout\n",
        "    for j in 100:#Nin\n",
        "      tmp = alpha * W2[i, j] * N\n",
        "      m_deltaX_accum(i, j) = beta1 * m_deltaX_accum(i, j) + beta1coef * (deltaX_accum(i, j) + tmp)\n",
        "      v_deltaX_accum(i, j) = beta2 * v_deltaX_accum(i, j) + beta2coef * (deltaX_accum(i, j) + tmp) * (deltaX_accum(i, j) + tmp)\n",
        "            \n",
        "    m_delta_accum(i) = beta1 * m_delta_accum(i) + beta1coef * delta_accum(i)\n",
        "    v_delta_accum(i) = beta2 * v_delta_accum(i) + beta2coef * delta_accum(i) * delta_accum(i)\n",
        "       \n",
        "        \n",
        "  sqbeta = Sqr((1 - beta2 ^ (epoch + 1)))#Sqrは平方根\n",
        "  learning_rate2 = learning_rate / (1 - beta1 ^ (epoch + 1)) * sqbeta\n",
        "  epsilon2 = sqbeta * epsilon\n",
        "        \n",
        "  for i in 100:\n",
        "    for j in 100:\n",
        "      W2[i, j] = W2[i, j] - learning_rate2 * m_deltaX_accum(i, j) / (Sqr(v_deltaX_accum(i, j)) + epsilon2) #重みの更新\n",
        "    B2[i] = B2[i] - learning_rate2 * m_delta_accum(i) / (Sqr(v_delta_accum(i)) + epsilon2)\n",
        "       \n",
        "  #print(\"W2\")\n",
        "  #print(W2)\n",
        "\n",
        "\n",
        "  ##バイアス\n",
        "  a = np.array([1,1,1,1])\n",
        "  sum_delta_bias = np.dot(a,delta)\n",
        "  delta_out_bias = 1/N*sum_delta_bias\n",
        "  #print(delta_out_bias)\n",
        "  #print(B2)\n",
        "  B2 = B2 - learning_rate*delta_out_bias\n",
        "  #print(\"B2\")\n",
        "  #print(B2)\n",
        "\n",
        "  ###隠れ層の逆伝搬\n",
        "  ##重み\n",
        "  differential_y = relu_diff(Z1)#活性化関数の微分\n",
        "  print(differential_y)\n",
        "  hid_delta = hidden_delta(delta,W2,differential_y)\n",
        "  #print(hid_delta)\n",
        "  x_t = np.transpose(x)\n",
        "  delta_hidden = 1/N*np.dot(x_t,hid_delta)\n",
        "  W1 = W1 - learning_rate*delta_hidden\n",
        "  #print(\"W1\")\n",
        "  #print(W1)\n",
        "\n",
        "  ##バイアス\n",
        "  a2 = np.array([1,1,1,1])\n",
        "  delta_hidden_bias = 1/N*np.dot(a2,hid_delta)\n",
        "  #print(B1)\n",
        "  B1 = B1 - learning_rate*delta_hidden_bias\n",
        "  #print(\"B1\")\n",
        "  #print(B1)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-bad79f4282cd>\"\u001b[0;36m, line \u001b[0;32m72\u001b[0m\n\u001b[0;31m    m_deltaX_accum(i, j) = beta1 * m_deltaX_accum(i, j) + beta1coef * (deltaX_accum(i, j) + tmp)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to function call\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XboY93W1UBMW"
      },
      "source": [
        "p = np.arange(300)\n",
        "plt.scatter(p, y, c='b', label='loss_data')\n",
        "#plt.legend()\n",
        "plt.title('loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGNWiiT6UDzY"
      },
      "source": [
        "A1 = np.dot(x,W1) +B1\n",
        "Z1 = relu(A1)#一層目活性化関数かける\n",
        "\n",
        "\n",
        "A2 = np.dot(Z1,W2)+B2\n",
        "Z22 = sigmoid(A2)\n",
        "\n",
        "delta = Z2 - target#誤差\n",
        "#print(delta)\n",
        "print(Z22)\n",
        "print(Z2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}